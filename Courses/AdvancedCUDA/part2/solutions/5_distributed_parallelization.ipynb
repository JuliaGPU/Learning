{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 4 - **Introduction to distributed parallelization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to:\n",
    "- understand the basic idea of distributed parallelization;\n",
    "- and learn about MPI.jl on the way.\n",
    "\n",
    "[*This content is distributed under MIT licence. Authors: S. Omlin (CSCS), L. Räss (ETHZ).*](https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/LICENSE.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: this course material is to a large part reusing material created by Ludovic Räss, ETH Zurich.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part notebook, we will explore distributed computing with Julia's MPI wrapper MPI.jl. This will enable our codes to run on multiple CPUs and GPUs in order to scale on modern multi-CPU/GPU nodes, clusters and supercomputers. In the proposed approach, each MPI process handles one CPU or GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we will look at the below 1-D diffusion code which solves the linear diffusion equations using a \"fake-parallelisation\" approach. We split the calculation on two distinct left and right domains, which requires left and right `H` arrays, `HL` and `HR`, respectively. In this \"fake parallelization\" code, the computations for the left and right domain are performed sequentially on one process, but they could be computed on two distinct processes if the needed boundary update (often referred to as halo update in literature) was done with MPI. The idea of this fake parallelization approach is the following:\n",
    "```julia\n",
    "# Compute physics locally\n",
    "HL[2:end-1] .= HL[2:end-1] .+ dt*λ*diff(diff(HL)/dx)/dx\n",
    "HR[2:end-1] .= HR[2:end-1] .+ dt*λ*diff(diff(HR)/dx)/dx\n",
    "# Update boundaries (later MPI)\n",
    "HL[end] = ...\n",
    "HR[1]   = ...\n",
    "# Global picture\n",
    "H .= [HL[1:end-1]; HR[2:end]]\n",
    "```\n",
    "We see that a correct boundary update will be the critical part for a successful implementation. In our approach, we need an overlap of 2 cells between `HL` and `HR` in order to avoid any wrong computations at the transition between the left and right domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (fake parallelization with 2 fake processes)\n",
    "\n",
    "Run the below \"fake parallelization\" 1-D diffusion code which is missing the boundary updates of the 2 fake processes and describe what you see in the visualization. Then, add the required boundary update in order make the code work properly and run it again. Note what has changed in the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "] activate ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "] instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "# Linear 1D diffusion with 2 fake mpi processes\n",
    "using Plots\n",
    "\n",
    "# enable plotting by default\n",
    "if !@isdefined do_visu; do_visu = true end\n",
    "\n",
    "@views function diffusion_1D_2procs(; do_visu=true)\n",
    "    # Physics\n",
    "    Hl  = 10.0   # left  H\n",
    "    Hr  = 1.0    # right H\n",
    "    λ   = 1.0    # diffusion coeff\n",
    "    nt  = 200    # number of time steps\n",
    "    # Numerics\n",
    "    nx  = 32     # number of local grid points\n",
    "    dx  = 1.0    # cell size\n",
    "    # Derived numerics\n",
    "    dt  = dx^2/λ/2.1\n",
    "    # Initial condition\n",
    "    HL  = Hl*ones(nx)\n",
    "    HR  = Hr*ones(nx)\n",
    "    H   = [HL[1:end-1]; HR[2:end]]\n",
    "    Hg  = copy(H)\n",
    "    # Time loop\n",
    "    @gif for it = 1:nt\n",
    "        # Compute physics locally\n",
    "        HL[2:end-1] .= HL[2:end-1] .+ dt*λ*diff(diff(HL)/dx)/dx\n",
    "        HR[2:end-1] .= HR[2:end-1] .+ dt*λ*diff(diff(HR)/dx)/dx\n",
    "        # Update boundaries\n",
    "        HL[end] = HR[2]\n",
    "        HR[1]   = HL[end-1]\n",
    "        # Global picture\n",
    "        H .= [HL[1:end-1]; HR[2:end]]\n",
    "        # Compute physics globally (check)\n",
    "        Hg[2:end-1] .= Hg[2:end-1] .+ dt*λ*diff(diff(Hg)/dx)/dx\n",
    "        # Visualise\n",
    "        if do_visu\n",
    "            fontsize = 12\n",
    "            plot(Hg, legend=false, linewidth=0, markershape=:circle, markersize=5, yaxis=font(fontsize, \"Courier\"), xaxis=font(fontsize, \"Courier\"), titlefontsize=fontsize, titlefont=\"Courier\")\n",
    "            plot!(H, legend=false, linewidth=3, framestyle=:box, xlabel=\"Lx\", ylabel=\"H\", title=\"diffusion (it=$(it))\")\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "diffusion_1D_2procs(; do_visu=do_visu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to generalise the fake parallelization with `2` fake processes to work with `n` fake processes. The idea of this generalized fake parallelization approach is the following:\n",
    "```julia\n",
    "for ip = 1:np    # compute physics locally\n",
    "    H[2:end-1,ip] .= H[2:end-1,ip] .+ dt*λ*diff(diff(H[:,ip])/dxg)/dxg\n",
    "end\n",
    "for ip = 1:np-1  # update boundaries\n",
    "    ...\n",
    "end\n",
    "for ip = 1:np    # global picture\n",
    "    i1 = 1 + (ip-1)*(nx-2)\n",
    "    Hg[i1:i1+nx-2] .= H[1:end-1,ip]\n",
    "end\n",
    "```\n",
    "The array `H` contains now `n` local domains where each domain belongs to one fake process, namely the fake process indicated by the second index of `H` (ip). The boundary updates are to be adapted accordingly. All the physical calculations happen on the local chunks of the arrays. We only need \"global\" knowledge in the definition of the initial condition, in order to e.g. initialise the step function used in the previous code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (fake parallelization with n fake processes)\n",
    "\n",
    "Modify the initial condition in the below \"fake parallelization\" 1-D diffusion code with `n` fake processes to match the initial condition of the previous code (a step function). Then run this code which is missing the boundary updates of the `n` fake processes and describe what you see in the visualization. Then, add the required boundary update in order make the code work properly and run it again. Note what has changed in the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "# Linear 1D diffusion with n fake mpi processes\n",
    "using Plots\n",
    "\n",
    "# enable plotting by default\n",
    "if !@isdefined do_visu; do_visu = true end\n",
    "\n",
    "@views function diffusion_1D_nprocs(; do_visu=true)\n",
    "    # Physics\n",
    "    lx  = 10.0\n",
    "    λ   = 1.0\n",
    "    nt  = 200\n",
    "    # Numerics\n",
    "    np  = 4             # number of procs\n",
    "    nx  = 32            # local number of grid points\n",
    "    # Derived numerics\n",
    "    nxg = (nx-2)*np+2   # global number of grid points\n",
    "    dxg = lx/nxg        # dx for global grid\n",
    "    dt  = dxg^2/λ/2.1\n",
    "    # Array allocation\n",
    "    x   = zeros(nx,np)  # local coord array\n",
    "    H   = zeros(nx,np)  # local H array\n",
    "    xt  = zeros(nxg)    # global coord array\n",
    "    Ht  = zeros(nxg)    # global initial H array\n",
    "    Hg  = zeros(nxg)    # global H array\n",
    "    # Initial condition\n",
    "    # ...\n",
    "    # Time loop\n",
    "    @gif for it = 1:nt\n",
    "        # Compute physics locally\n",
    "        for ip = 1:np\n",
    "            H[2:end-1,ip] .= H[2:end-1,ip] .+ dt*λ*diff(diff(H[:,ip])/dxg)/dxg\n",
    "        end\n",
    "        # Update boundaries\n",
    "        for ip = 1:np-1\n",
    "            H[end,ip  ] = H[    2,ip+1]\n",
    "            H[  1,ip+1] = H[end-1,ip  ]\n",
    "        end\n",
    "        # Global picture\n",
    "        for ip = 1:np\n",
    "            i1 = 1 + (ip-1)*(nx-2)\n",
    "            Hg[i1:i1+nx-2] .= H[1:end-1,ip]\n",
    "        end\n",
    "        # Visualise\n",
    "        if do_visu\n",
    "            fontsize = 12\n",
    "            plot(xt, Ht, legend=false, linewidth=1, markershape=:circle, markersize=3, yaxis=font(fontsize, \"Courier\"), xaxis=font(fontsize, \"Courier\"), titlefontsize=fontsize, titlefont=\"Courier\")\n",
    "            plot!(xt, Hg, legend=false, linewidth=3, framestyle=:box, xlabel=\"Lx\", ylabel=\"H\", title=\"diffusion (it=$(it))\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "diffusion_1D_nprocs(; do_visu=do_visu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous simple initial conditions can be easily defined without computing any Carthesian coordinates. To define other initial conditions we often need to compute global coordinates. In the code below, which serves to define a Gaussian anomaly in the center of the domain, they can be computed for each cell based on the process id (`ip`), the cell id (`ix`), the array size (`nx`), the overlap of the local domains (`2`) and the grid spacing of the global grid (`dxg`); moreover, the origin of the coordinate system can be moved to any position using the global domain length (`lx`):\n",
    "```julia\n",
    "# Initial condition\n",
    "for ip = 1:np\n",
    "    for ix = 1:nx\n",
    "        x[ix,ip] = ...\n",
    "        H[ix,ip] = exp(-x[ix,ip]^2)\n",
    "    end\n",
    "    i1 = 1 + (ip-1)*(nx-2)\n",
    "    xt[i1:i1+nx-2] .= x[1:end-1,ip]; if (ip==np) xt[i1+nx-1] = x[end,ip] end\n",
    "    Ht[i1:i1+nx-2] .= H[1:end-1,ip]; if (ip==np) Ht[i1+nx-1] = H[end,ip] end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (fake parallelization with n fake processes)\n",
    "\n",
    "Modify the initial condition in the below code (which is the same as above) to a centered Gaussian anomaly. Then run this code which is missing the boundary updates of the `n` fake processes and describe what you see in the visualization. Then, add the required boundary update in order make the code work properly and run it again. Note what has changed in the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "# Linear 1D diffusion with n fake mpi processes\n",
    "using Plots\n",
    "\n",
    "# enable plotting by default\n",
    "if !@isdefined do_visu; do_visu = true end\n",
    "\n",
    "@views function diffusion_1D_nprocs(; do_visu=true)\n",
    "    # Physics\n",
    "    lx  = 10.0\n",
    "    λ   = 1.0\n",
    "    nt  = 200\n",
    "    # Numerics\n",
    "    np  = 4             # number of procs\n",
    "    nx  = 32            # local number of grid points\n",
    "    # Derived numerics\n",
    "    nxg = (nx-2)*np+2   # global number of grid points\n",
    "    dxg = lx/nxg        # dx for global grid\n",
    "    dt  = dxg^2/λ/2.1\n",
    "    # Array allocation\n",
    "    x   = zeros(nx,np)  # local coord array\n",
    "    H   = zeros(nx,np)  # local H array\n",
    "    xt  = zeros(nxg)    # global coord array\n",
    "    Ht  = zeros(nxg)    # global initial H array\n",
    "    Hg  = zeros(nxg)    # global H array\n",
    "    # Initial condition\n",
    "    for ip = 1:np\n",
    "        i1 = 1 + (ip-1)*(nx-2)\n",
    "        for ix = 1:nx\n",
    "            x[ix,ip] = ( (ip-1)*(nx-2) + (ix-0.5) )*dxg - 0.5*lx\n",
    "            H[ix,ip] = exp(-x[ix,ip]^2)\n",
    "        end\n",
    "        xt[i1:i1+nx-2] .= x[1:end-1,ip]; if (ip==np) xt[i1+nx-1] = x[end,ip] end\n",
    "        Ht[i1:i1+nx-2] .= H[1:end-1,ip]; if (ip==np) Ht[i1+nx-1] = H[end,ip] end\n",
    "    end\n",
    "    # Time loop\n",
    "    @gif for it = 1:nt\n",
    "        # Compute physics locally\n",
    "        for ip = 1:np\n",
    "            H[2:end-1,ip] .= H[2:end-1,ip] .+ dt*λ*diff(diff(H[:,ip])/dxg)/dxg\n",
    "        end\n",
    "        # Update boundaries\n",
    "        for ip = 1:np-1\n",
    "            H[end,ip  ] = H[    2,ip+1]\n",
    "            H[  1,ip+1] = H[end-1,ip  ]\n",
    "        end\n",
    "        # Global picture\n",
    "        for ip = 1:np\n",
    "            i1 = 1 + (ip-1)*(nx-2)\n",
    "            Hg[i1:i1+nx-2] .= H[1:end-1,ip]\n",
    "        end\n",
    "        # Visualise\n",
    "        if do_visu\n",
    "            fontsize = 12\n",
    "            plot(xt, Ht, legend=false, linewidth=1, markershape=:circle, markersize=3, yaxis=font(fontsize, \"Courier\"), xaxis=font(fontsize, \"Courier\"), titlefontsize=fontsize, titlefont=\"Courier\")\n",
    "            plot!(xt, Hg, legend=false, linewidth=3, framestyle=:box, xlabel=\"Lx\", ylabel=\"H\", title=\"diffusion (it=$(it))\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "diffusion_1D_nprocs(; do_visu=do_visu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to write a code that will truly distribute calculations on different processors using MPI.jl.\n",
    "Let us see what are the somewhat minimal requirements that will allow us to write a distributed code in Julia using MPI.jl. We will solve the following linear diffusion physics:\n",
    "```julia\n",
    "for it = 1:nt\n",
    "    qHx        .= .-λ*diff(H)/dx\n",
    "    H[2:end-1] .= H[2:end-1] .- dt*diff(qHx)/dx\n",
    "end\n",
    "```\n",
    "To enable distributed parallelization, we will do the following steps:\n",
    "1. Initialise MPI and set up a Cartesian communicator\n",
    "2. Implement a boundary exchange routine\n",
    "3. Create a \"global\" initial condition\n",
    "4. Finalise MPI\n",
    "\n",
    "To (1.) initialise MPI and prepare the Cartesian communicator, we do:\n",
    "````julia\n",
    "MPI.Init()\n",
    "dims        = [0]\n",
    "comm        = MPI.COMM_WORLD\n",
    "nprocs      = MPI.Comm_size(comm)\n",
    "MPI.Dims_create!(nprocs, dims)\n",
    "comm_cart   = MPI.Cart_create(comm, dims, [0], 1)\n",
    "me          = MPI.Comm_rank(comm_cart)\n",
    "coords      = MPI.Cart_coords(comm_cart)\n",
    "neighbors_x = MPI.Cart_shift(comm_cart, 0, 1)\n",
    "````\n",
    "where `me` represents the process ID unique to each MPI process (the analogue to `ip` in the fake parallelization).\n",
    "\n",
    "Then, we need to (2.) implement a boundary update routine, which can have the following structure:\n",
    "```julia\n",
    "@views function update_halo(A, neighbors_x, comm)\n",
    "    # Send to / receive from neighbor 1 (\"left neighbor\")\n",
    "    if neighbors_x[1] != MPI.MPI_PROC_NULL\n",
    "        # ...\n",
    "    end\n",
    "    # Send to / receive from neighbor 2 (\"right neighbor\")\n",
    "    if neighbors_x[2] != MPI.MPI_PROC_NULL\n",
    "        # ...\n",
    "    end\n",
    "    return\n",
    "end\n",
    "```\n",
    "\n",
    "Then, we (3.) initialize `H` with a \"global\" initial Gaussian anomaly that spans correctly over all local domains. This can be achieved, e.g., as given here:\n",
    "```julia\n",
    "x0    = coords[1]*(nx-2)*dx\n",
    "xc    = [x0 + ix*dx - dx/2 - 0.5*lx  for ix=1:nx]\n",
    "H     = exp.(.-xc.^2)\n",
    "```\n",
    "where `x0` represents the first global x-coordinate on every process (computed in function of `coords`) and `xc` represents the local chunk of the global coordinates on each local process (this is analogue to the initialization in the fake parallelization).\n",
    "\n",
    "Last, we need to (4.) finalise MPI prior to returning from the main function:\n",
    "```julia\n",
    "MPI.Finalize()\n",
    "```\n",
    "All the above described is found in the code `diffusion_1D_mpi.jl`, except for the boundary updates (see 2.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (1-D parallelization with MPI)\n",
    "\n",
    "Run the code `diffusion_1D_mpi.jl` which is still missing the boundary updates three times: with 1, 2 and 4 processes; visualize the results after each run with the code that follows (adapt the variable nprocs!). Describe what you see in the visualization. Then, add the required boundary update in order make the code work properly and run it again. Note what has changed in the visualization.\n",
    "> 💡 Hint: for the boundary updates, you can use the following approach for the communication with each neighbor: 1) create a sendbuffer and receive buffer, storing the right value in the send buffer; 2) use `MPI.Send` and `MPI.Recv!` to send/reveive the data; 3) store the received data in the right position in the Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "# Visualisation script for the 1D MPI solver\n",
    "using Plots, MAT\n",
    "\n",
    "nprocs = 2\n",
    "\n",
    "@views function vizme1D_mpi(nprocs)\n",
    "    H = []\n",
    "    for ip = 1:nprocs\n",
    "        file = matopen(\"H_$(ip-1).mat\"); H_loc = read(file, \"H\"); close(file)\n",
    "        nx_i = length(H_loc)-2\n",
    "        i1   = 1 + (ip-1)*nx_i\n",
    "        if (ip==1)  H = zeros(nprocs*nx_i)  end\n",
    "        H[i1:i1+nx_i-1] .= H_loc[2:end-1]\n",
    "    end\n",
    "    fontsize = 12\n",
    "    display(plot(H, legend=false, framestyle=:box, linewidth=3, xlims=(1, length(H)), ylims=(0, 1), xlabel=\"nx\", title=\"diffusion 1D MPI\", yaxis=font(fontsize, \"Courier\"), xaxis=font(fontsize, \"Courier\"), titlefontsize=fontsize, titlefont=\"Courier\"))\n",
    "    return\n",
    "end\n",
    "\n",
    "vizme1D_mpi(nprocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just did a distributed diffusion solver in only 70 lines of code.\n",
    "Let us now do the same in 2-D: there is not much new there, but it may be interesting to work out how boundary update routines can be defined in 2D as one now needs to exchange vectors instead of single values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (2-D parallelization with MPI)\n",
    "\n",
    "Run the code `diffusion_2D_mpi.jl` which is still missing the boundary updates three times: with 1, 2 and 4 processes; visualize the results after each run with the code that follows (adapt the variable nprocs!). Describe what you see in the visualization. Then, add the required boundary update in order make the code work properly and run it again. Note what has changed in the visualization.\n",
    "> 💡 Hint: for the boudnary updates, you can use the following approach for the communication with each neighbor: 1) create a sendbuffer and receive buffer, storing the right value in the send buffer; 2) use `MPI.Send` and `MPI.Recv!` to send/reveive the data; 3) store the received data in the right position in the Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "# Visualisation script for the 2D MPI solver\n",
    "using Plots, MAT\n",
    "\n",
    "nprocs = (2, 1) # nprocs (x, y) dim\n",
    "\n",
    "@views function vizme2D_mpi(nprocs)\n",
    "    H  = []\n",
    "    ip = 1\n",
    "    for ipx = 1:nprocs[1]\n",
    "        for ipy = 1:nprocs[2]\n",
    "            file = matopen(\"H_$(ip-1).mat\"); H_loc = read(file, \"H\"); close(file)\n",
    "            nx_i, ny_i = size(H_loc,1)-2, size(H_loc,2)-2\n",
    "            ix1, iy1   = 1+(ipx-1)*nx_i, 1+(ipy-1)*ny_i\n",
    "            if (ip==1)  H = zeros(nprocs[1]*nx_i, nprocs[2]*ny_i)  end\n",
    "            H[ix1:ix1+nx_i-1,iy1:iy1+ny_i-1] .= H_loc[2:end-1,2:end-1]\n",
    "            ip += 1\n",
    "        end\n",
    "    end\n",
    "    fontsize = 12\n",
    "    opts = (aspect_ratio=1, yaxis=font(fontsize, \"Courier\"), xaxis=font(fontsize, \"Courier\"),\n",
    "        ticks=nothing, framestyle=:box, titlefontsize=fontsize, titlefont=\"Courier\",\n",
    "        xlabel=\"Lx\", ylabel=\"Ly\", xlims=(1, size(H,1)), ylims=(1, size(H,2)) )\n",
    "    display(heatmap(H'; c=:davos, title=\"diffusion 2D MPI\", opts...))\n",
    "    return\n",
    "end\n",
    "\n",
    "vizme2D_mpi(nprocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that an HPC application will normally use non-blocking communication. That means that instead of `MPI.Send` and `MPI.Recv!` one can use `MPI.Isend` and `MPI.Irecv!` to asynchronously launch the communication and wait then at the end of the boundary updates with `MPI.Wait!` or `MPI.Waitall!` for completion of the communication. Note also, that in an HPC application, the receiving calls should be made before the sending calls, in order to have the receiver ready when the sender wants to start to send (else the sender will have to write first into a buffer). Finally, note that the buffer allocations in the update_boundary functions would better be replaced by usage of preallocated buffers.\n",
    "\n",
    "Our last step in this introduction notebook is to create a multi-GPU solver out of the above multi-CPU solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (multi-GPU)\n",
    "\n",
    "Translate the code `diffusion_2D_mpi.jl` from Task 4 to GPU using GPU array programming. Note what changes were needed to go from CPU to GPU in this distributed solver.\n",
    "> 💡 Hint: use `copyto!` to copy the data to send from the device into a sendbuffer on the host; use `copyto!` also to copy received data back to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "# Visualisation script for the 2D MPI solver\n",
    "using Plots, MAT\n",
    "\n",
    "nprocs = (2, 1) # nprocs (x, y) dim\n",
    "\n",
    "@views function vizme2D_mpi(nprocs)\n",
    "    H  = []\n",
    "    ip = 1\n",
    "    for ipx = 1:nprocs[1]\n",
    "        for ipy = 1:nprocs[2]\n",
    "            file = matopen(\"H_$(ip-1).mat\"); H_loc = read(file, \"H\"); close(file)\n",
    "            nx_i, ny_i = size(H_loc,1)-2, size(H_loc,2)-2\n",
    "            ix1, iy1   = 1+(ipx-1)*nx_i, 1+(ipy-1)*ny_i\n",
    "            if (ip==1)  H = zeros(nprocs[1]*nx_i, nprocs[2]*ny_i)  end\n",
    "            H[ix1:ix1+nx_i-1,iy1:iy1+ny_i-1] .= H_loc[2:end-1,2:end-1]\n",
    "            ip += 1\n",
    "        end\n",
    "    end\n",
    "    fontsize = 12\n",
    "    opts = (aspect_ratio=1, yaxis=font(fontsize, \"Courier\"), xaxis=font(fontsize, \"Courier\"),\n",
    "        ticks=nothing, framestyle=:box, titlefontsize=fontsize, titlefont=\"Courier\",\n",
    "        xlabel=\"Lx\", ylabel=\"Ly\", xlims=(1, size(H,1)), ylims=(1, size(H,2)) )\n",
    "    display(heatmap(H'; c=:davos, title=\"diffusion 2D MPI\", opts...))\n",
    "    return\n",
    "end\n",
    "\n",
    "vizme2D_mpi(nprocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further step would be to use CUDA-aware MPI, which allows to avoid these explicit copies from host to device. It permits to pass directly GPU memory pointers to the MPI functions. CUDA-aware MPI enables to leverage Remote Direct Memory Access (RDMA) which can be of great benefit in many HPC scenarios.\n",
    "\n",
    "This completes the introduction to distributed parallelization with Julia. Note that the highlevel Julia HPC package [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl) renders the distributed parallelization of stencil codes with GPU and CPU an almost trivial task and enables close to ideal weak scaling of real-world applications on thousands of GPUs. As little as three functions can be enough to transform a single GPU/CPU application into a massively scaling Multi-GPU/CPU application (see for example [here](https://github.com/eth-cscs/ImplicitGlobalGrid.jl/blob/master/examples/diffusion3D_multigpu_CuArrays_novis.jl)). The documentation of  [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl) provides more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
